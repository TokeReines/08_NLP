{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokereines/miniconda3/envs/08_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from bpemb import BPEmb\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "# Define the languages of interest\n",
    "languages = [\"arabic\", \"bengali\", \"indonesian\"]\n",
    "language_to_bpe = {'bengali': BPEmb(lang=\"bn\", dim=50), 'indonesian': BPEmb(lang=\"id\", dim=50), 'arabic': BPEmb(lang=\"ar\", dim=50)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_set.filter(lambda example: example['language'] == 'indonesian' and example['document_plaintext'] is not None)[0]\n",
    "sample_row['document_plaintext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing using BPEmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokens: ['▁ern', 'est', '▁d', 'ou', 'w', 'es', '▁dek', 'ker', '▁wafat', '▁dini']\n"
     ]
    }
   ],
   "source": [
    "bpemb_model = language_to_bpe[sample_row['language']]\n",
    "\n",
    "question_bpe = bpemb_model.encode(sample_row['question_text'])\n",
    "document_bpe = bpemb_model.encode(sample_row['document_plaintext'])\n",
    "print(\"BPE tokens:\", document_bpe[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 tokens: ['Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "\n",
    "sample_question_gpt2 = tokenizer.tokenize(sample_row['question_text'])\n",
    "sample_document_gpt2 = tokenizer.tokenize(sample_row['document_plaintext'])\n",
    "print(\"GPT2 tokens:\", sample_document_gpt2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "n = 3\n",
    "\n",
    "padded_text = list(pad_both_ends(sample_document_gpt2, n=n))\n",
    "print(padded_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '<s>', 'Er'), ('<s>', 'Er', 'nest'), ('Er', 'nest', 'ĠD'), ('nest', 'ĠD', 'ouw'), ('ĠD', 'ouw', 'es'), ('ouw', 'es', 'ĠDek'), ('es', 'ĠDek', 'ker'), ('ĠDek', 'ker', 'Ġwafat'), ('ker', 'Ġwafat', 'Ġdini'), ('Ġwafat', 'Ġdini', 'Ġhari'), ('Ġdini', 'Ġhari', 'Ġtanggal'), ('Ġhari', 'Ġtanggal', 'Ġ28'), ('Ġtanggal', 'Ġ28', 'ĠAgustus'), ('Ġ28', 'ĠAgustus', 'Ġ1950'), ('ĠAgustus', 'Ġ1950', 'Ġ('), ('Ġ1950', 'Ġ(', 'ter'), ('Ġ(', 'ter', 'tulis'), ('ter', 'tulis', 'Ġdi'), ('tulis', 'Ġdi', 'Ġbatu'), ('Ġdi', 'Ġbatu', 'Ġnis'), ('Ġbatu', 'Ġnis', 'annya'), ('Ġnis', 'annya', ';'), ('annya', ';', 'Ġ29'), (';', 'Ġ29', 'ĠAgustus'), ('Ġ29', 'ĠAgustus', 'Ġ1950'), ('ĠAgustus', 'Ġ1950', 'Ġversi'), ('Ġ1950', 'Ġversi', 'Ġvan'), ('Ġversi', 'Ġvan', 'Ġder'), ('Ġvan', 'Ġder', 'ĠV'), ('Ġder', 'ĠV', 'eur'), ('ĠV', 'eur', ','), ('eur', ',', 'Ġ2006'), (',', 'Ġ2006', ')'), ('Ġ2006', ')', 'Ġdan'), (')', 'Ġdan', 'Ġdimakamkan'), ('Ġdan', 'Ġdimakamkan', 'Ġdi'), ('Ġdimakamkan', 'Ġdi', 'ĠT'), ('Ġdi', 'ĠT', 'MP'), ('ĠT', 'MP', 'ĠC'), ('MP', 'ĠC', 'ikut'), ('ĠC', 'ikut', 'ra'), ('ikut', 'ra', ','), ('ra', ',', 'ĠBandung'), (',', 'ĠBandung', '.'), ('ĠBandung', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "padded_n_grams = ngrams(padded_text, n=n)\n",
    "print(list(padded_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari', 'Ġtanggal', 'Ġ28', 'ĠAgustus', 'Ġ1950', 'Ġversi', 'Ġvan', 'Ġder', 'ĠV', 'eur', ',', 'Ġ2006']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [sample_document_gpt2])\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for arabic using 1-grams in question: 1113.8889298334905\n",
      "Perplexity for arabic using 2-grams in question: 806.2036869725847\n",
      "Perplexity for arabic using 3-grams in question: 2892.8206527586626\n",
      "Perplexity for bengali using 1-grams in question: 1033.544124486604\n",
      "Perplexity for bengali using 2-grams in question: 392.7413803882843\n",
      "Perplexity for bengali using 3-grams in question: 907.161208880907\n",
      "Perplexity for indonesian using 1-grams in question: 1054.0973057955289\n",
      "Perplexity for indonesian using 2-grams in question: 690.5564623641836\n",
      "Perplexity for indonesian using 3-grams in question: 2156.901599065793\n",
      "Perplexity for arabic using 1-grams document: 2361.35061286874\n",
      "Perplexity for arabic using 2-grams document: 1057.5043929730352\n",
      "Perplexity for arabic using 3-grams document: 4318.201644807369\n",
      "Perplexity for bengali using 1-grams document: 2322.6386501576\n",
      "Perplexity for bengali using 2-grams document: 2030.7911510277181\n",
      "Perplexity for bengali using 3-grams document: 5243.607188592675\n",
      "Perplexity for indonesian using 1-grams document: 1979.651081734886\n",
      "Perplexity for indonesian using 2-grams document: 1715.340691101209\n",
      "Perplexity for indonesian using 3-grams document: 5783.023270886824\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import ngrams\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "# Define the languages of interest\n",
    "languages = [\"arabic\", \"bengali\", \"indonesian\"]\n",
    "language_to_bpe = {\n",
    "    'bengali': BPEmb(lang=\"bn\", dim=50), \n",
    "    'indonesian': BPEmb(lang=\"id\", dim=50), \n",
    "    'arabic': BPEmb(lang=\"ar\", dim=50)\n",
    "}\n",
    "\n",
    "# Define an order for the n-grams\n",
    "N = 3\n",
    "\n",
    "def tokenize_dataset(dataset, lang, language_to_bpe, question_only=False, document_only=False):\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        if entry['language'] != lang:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if question_only:\n",
    "            text = entry['question_text']\n",
    "        elif document_only:\n",
    "            text = entry['document_plaintext']\n",
    "        else:\n",
    "            text = entry['question_text'] + ' ' + entry['document_plaintext']\n",
    "            \n",
    "        bpe_text = language_to_bpe[lang].encode(text)\n",
    "        tokenized_data.append(bpe_text)\n",
    "    return tokenized_data\n",
    "\n",
    "# Create and evaluate a Laplace model for each language\n",
    "for lang in languages:\n",
    "    for n in range(1, N+1):\n",
    "        tokenized_train = tokenize_dataset(train_set, lang, language_to_bpe, question_only=True)\n",
    "        tokenized_validation = tokenize_dataset(validation_set, lang, language_to_bpe, question_only=True)\n",
    "\n",
    "        train_data, padded_vocab = padded_everygram_pipeline(N, tokenized_train)\n",
    "        validation_data, _ = padded_everygram_pipeline(N, tokenized_validation)\n",
    "\n",
    "        lm = Laplace(n)\n",
    "        lm.fit(train_data, padded_vocab)\n",
    "\n",
    "        # Calculate perplexity on validation data for the current language\n",
    "        validation_ngrams = [ng for sent in tokenized_validation for ng in ngrams(sent, n)]\n",
    "        perplexity = lm.perplexity(validation_ngrams)\n",
    "        print(f\"Perplexity for {lang} using {n}-grams in question: {perplexity}\")\n",
    "        \n",
    "\n",
    "# Create and evaluate a Laplace model for each language\n",
    "for lang in languages:\n",
    "    for n in range(1, N+1):\n",
    "        tokenized_train = tokenize_dataset(train_set, lang, language_to_bpe, document_only=True)\n",
    "        tokenized_validation = tokenize_dataset(validation_set, lang, language_to_bpe, document_only=True)\n",
    "\n",
    "        train_data, padded_vocab = padded_everygram_pipeline(N, tokenized_train)\n",
    "        validation_data, _ = padded_everygram_pipeline(N, tokenized_validation)\n",
    "\n",
    "        lm = Laplace(n)\n",
    "        lm.fit(train_data, padded_vocab)\n",
    "\n",
    "        # Calculate perplexity on validation data for the current language\n",
    "        validation_ngrams = [ng for sent in tokenized_validation for ng in ngrams(sent, n)]\n",
    "        perplexity = lm.perplexity(validation_ngrams)\n",
    "        print(f\"Perplexity for {lang} using {n}-grams document: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "languages = ['indonesian',]\n",
    "n = 3\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in train_set if row['language'] == language))\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [gpt2_tokens])\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21322.709156468547\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in validation_set if row['language'] == language))\n",
    "padded_tokens_validation = list(pad_sequence(gpt2_tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=n))\n",
    "\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = model.perplexity(ngrams(padded_tokens_validation, n=n))\n",
    "print(perplexity)\n",
    "# list(bigrams(padded_tokens_validation))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8512773722627736e-05\n",
      "35072\n"
     ]
    }
   ],
   "source": [
    "print(model.score(\"<s>\", \"Kol\"))\n",
    "print(len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "languages = ['indonesian',]\n",
    "n = 3\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in train_set if row['language'] == language))\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [gpt2_tokens])\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
