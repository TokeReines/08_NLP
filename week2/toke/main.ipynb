{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from bpemb import BPEmb\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "# Define the languages of interest\n",
    "languages = [\"arabic\", \"bengali\", \"indonesian\"]\n",
    "language_to_bpe = {'bengali': BPEmb(lang=\"bn\", dim=50), 'indonesian': BPEmb(lang=\"id\", dim=50), 'arabic': BPEmb(lang=\"ar\", dim=50)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_set.filter(lambda example: example['language'] == 'indonesian' and example['document_plaintext'] is not None)[0]\n",
    "sample_row['document_plaintext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing using BPEmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokens: ['▁ern', 'est', '▁d', 'ou', 'w', 'es', '▁dek', 'ker', '▁wafat', '▁dini']\n"
     ]
    }
   ],
   "source": [
    "bpemb_model = language_to_bpe[sample_row['language']]\n",
    "\n",
    "question_bpe = bpemb_model.encode(sample_row['question_text'])\n",
    "document_bpe = bpemb_model.encode(sample_row['document_plaintext'])\n",
    "print(\"BPE tokens:\", document_bpe[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 tokens: ['Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "\n",
    "sample_question_gpt2 = tokenizer.tokenize(sample_row['question_text'])\n",
    "sample_document_gpt2 = tokenizer.tokenize(sample_row['document_plaintext'])\n",
    "print(\"GPT2 tokens:\", sample_document_gpt2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "n = 3\n",
    "\n",
    "padded_text = list(pad_both_ends(sample_document_gpt2, n=n))\n",
    "print(padded_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '<s>', 'Er'), ('<s>', 'Er', 'nest'), ('Er', 'nest', 'ĠD'), ('nest', 'ĠD', 'ouw'), ('ĠD', 'ouw', 'es'), ('ouw', 'es', 'ĠDek'), ('es', 'ĠDek', 'ker'), ('ĠDek', 'ker', 'Ġwafat'), ('ker', 'Ġwafat', 'Ġdini'), ('Ġwafat', 'Ġdini', 'Ġhari'), ('Ġdini', 'Ġhari', 'Ġtanggal'), ('Ġhari', 'Ġtanggal', 'Ġ28'), ('Ġtanggal', 'Ġ28', 'ĠAgustus'), ('Ġ28', 'ĠAgustus', 'Ġ1950'), ('ĠAgustus', 'Ġ1950', 'Ġ('), ('Ġ1950', 'Ġ(', 'ter'), ('Ġ(', 'ter', 'tulis'), ('ter', 'tulis', 'Ġdi'), ('tulis', 'Ġdi', 'Ġbatu'), ('Ġdi', 'Ġbatu', 'Ġnis'), ('Ġbatu', 'Ġnis', 'annya'), ('Ġnis', 'annya', ';'), ('annya', ';', 'Ġ29'), (';', 'Ġ29', 'ĠAgustus'), ('Ġ29', 'ĠAgustus', 'Ġ1950'), ('ĠAgustus', 'Ġ1950', 'Ġversi'), ('Ġ1950', 'Ġversi', 'Ġvan'), ('Ġversi', 'Ġvan', 'Ġder'), ('Ġvan', 'Ġder', 'ĠV'), ('Ġder', 'ĠV', 'eur'), ('ĠV', 'eur', ','), ('eur', ',', 'Ġ2006'), (',', 'Ġ2006', ')'), ('Ġ2006', ')', 'Ġdan'), (')', 'Ġdan', 'Ġdimakamkan'), ('Ġdan', 'Ġdimakamkan', 'Ġdi'), ('Ġdimakamkan', 'Ġdi', 'ĠT'), ('Ġdi', 'ĠT', 'MP'), ('ĠT', 'MP', 'ĠC'), ('MP', 'ĠC', 'ikut'), ('ĠC', 'ikut', 'ra'), ('ikut', 'ra', ','), ('ra', ',', 'ĠBandung'), (',', 'ĠBandung', '.'), ('ĠBandung', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "padded_n_grams = ngrams(padded_text, n=n)\n",
    "print(list(padded_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari', 'Ġtanggal', 'Ġ28', 'ĠAgustus', 'Ġ1950', 'Ġversi', 'Ġvan', 'Ġder', 'ĠV', 'eur', ',', 'Ġ2006']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [sample_document_gpt2])\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "language = 'indonesian'\n",
    "n = 3\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in train_set if row['language'] == language))\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [gpt2_tokens])\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21322.709156468547\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in validation_set if row['language'] == language))\n",
    "padded_tokens_validation = list(pad_sequence(gpt2_tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=n))\n",
    "\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = model.perplexity(ngrams(padded_tokens_validation, n=n))\n",
    "print(perplexity)\n",
    "# list(bigrams(padded_tokens_validation))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8512773722627736e-05\n",
      "35072\n"
     ]
    }
   ],
   "source": [
    "print(model.score(\"<s>\", \"Kol\"))\n",
    "print(len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
