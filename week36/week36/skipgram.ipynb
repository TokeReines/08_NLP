{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "8_EFyXUTWY7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41880f3e-0078-4a70-fd4b-d2141fbcabe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "2023-10-02 11:08:18.667440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install bpemb\n",
        "!pip install gensim\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install datasets\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ftt1eBtX2pb",
        "outputId": "a4de00a2-5aab-4a78-dca2-41f06d44ce60"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]"
      ],
      "metadata": {
        "id": "PeMM5RcyZj7E"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rows = train_set.filter(lambda example: example['language'] == 'arabic')\n",
        "train_rows = sample_rows['document_plaintext']\n",
        "train_rows = train_rows[:100]\n",
        "\n",
        "validation_rows = validation_set.filter(lambda example: example['language'] == 'arabic')\n",
        "validation_rows = validation_rows['document_plaintext']\n",
        "validation_rows = validation_rows[:2]"
      ],
      "metadata": {
        "id": "nb6gMqoKhiEp"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rows[0]"
      ],
      "metadata": {
        "id": "aHiE9Wjtic_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ac6727b8-18a6-462f-ae26-ad76d4636365"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nالمسألة الشرقية (بالإنجليزية: Eastern Question) (بالفرنسية: Question de l'orient): هي مسألة وجود العثمانيين المسلمين في أوروبا وطردهم منها واستعادة القسطنطينية من العثمانيين بعد سقوطها في 1453 وتهديد مصالح الدول الأوروبية في هذه المنطقة. كما يدل المصطلح على تصفية أملاك رجل أوروبا المريض في البلقان من طرف الدول الأوروبية.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = input_data.lower()\n",
        "    lowercase = re.sub('\\[\\d+\\]', ' ', lowercase)\n",
        "    lowercase = re.sub('[%s]' % re.escape(string.punctuation), '', lowercase)\n",
        "    return lowercase"
      ],
      "metadata": {
        "id": "rXGZzdbalOUw"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def split_paragraphs_into_sentences(paragraphs):\n",
        "    sentences = []\n",
        "    for paragraph in paragraphs:\n",
        "        sentences.extend(sent_tokenize(paragraph))\n",
        "    return sentences\n",
        "\n",
        "sentences = split_paragraphs_into_sentences(train_rows)\n",
        "validation_sentences = split_paragraphs_into_sentences(validation_rows)"
      ],
      "metadata": {
        "id": "XdWdWzr9mkzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0eb02e-4248-40fd-c421-6ac1037ec156"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_sentences = [custom_standardization(sentence) for sentence in sentences]\n",
        "filtered_sentences = [sentence for sentence in standardized_sentences if len(sentence.split()) > 2]\n",
        "\n",
        "standardized_validation_sentences = [custom_standardization(sentence) for sentence in validation_sentences]\n",
        "filtered_validation_sentences = [sentence for sentence in standardized_validation_sentences if len(sentence.split()) > 2]"
      ],
      "metadata": {
        "id": "90-bFVtywT8r"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentences[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcEAK2_Mpa64",
        "outputId": "68bfb954-0799-437d-f7fe-2bd84b1cf954"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\\nالمسألة الشرقية بالإنجليزية eastern question بالفرنسية question de lorient هي مسألة وجود العثمانيين المسلمين في أوروبا وطردهم منها واستعادة القسطنطينية من العثمانيين بعد سقوطها في 1453 وتهديد مصالح الدول الأوروبية في هذه المنطقة',\n",
              " 'كما يدل المصطلح على تصفية أملاك رجل أوروبا المريض في البلقان من طرف الدول الأوروبية']"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [word for sentence in filtered_sentences for word in sentence.split()]\n",
        "vocab.append('<UNK>')\n",
        "unique_vocab = list(set(vocab))\n",
        "word2idx = {w: idx for (idx, w) in enumerate(unique_vocab)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(word2idx)}\n",
        "vocabulary_size = len(unique_vocab)"
      ],
      "metadata": {
        "id": "VYrWQKSQ5K0x"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_idx_pairs(input_sentences):\n",
        "  window_size = 2\n",
        "  idx_pairs = []\n",
        "  for sen in input_sentences:\n",
        "      indices = [word2idx[word] for sentence in input_sentences for word in sentence.split()]\n",
        "      # for each word, threated as center word\n",
        "      for center_word_pos in range(len(indices)):\n",
        "          # for each window position\n",
        "          for w in range(-window_size, window_size + 1):\n",
        "              context_word_pos = center_word_pos + w\n",
        "              # make soure not jump out sentence\n",
        "              if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                  continue\n",
        "              context_word_idx = indices[context_word_pos]\n",
        "              idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "  idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "  return idx_pairs\n",
        "#define a function for the previous task\n"
      ],
      "metadata": {
        "id": "snHtEmuv9Vfo"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idx_pairs(input_sentences, word2idx):\n",
        "  window_size = 2\n",
        "  idx_pairs = []\n",
        "  for sen in input_sentences:\n",
        "      indices = [word2idx.get(word, word2idx['<UNK>']) for word in sen.split()]\n",
        "      # for each word, treated as center word\n",
        "      for center_word_pos in range(len(indices)):\n",
        "          # for each window position\n",
        "          for w in range(-window_size, window_size + 1):\n",
        "              context_word_pos = center_word_pos + w\n",
        "              # make sure not jump out sentence\n",
        "              if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                  continue\n",
        "              context_word_idx = indices[context_word_pos]\n",
        "              idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "  idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "  return idx_pairs"
      ],
      "metadata": {
        "id": "vtRCTMFZC-ee"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_pairs = get_idx_pairs(filtered_sentences, word2idx)\n",
        "idx_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_WdksEU_tHo",
        "outputId": "1c76d18a-8c47-4244-bb80-304e485a7df7"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2477, 1370],\n",
              "       [2477, 4297],\n",
              "       [1370, 2477],\n",
              "       ...,\n",
              "       [ 807, 4409],\n",
              "       [4409, 1336],\n",
              "       [4409,  807]])"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_pairs_eval = get_idx_pairs(filtered_validation_sentences, word2idx)\n",
        "idx_pairs_eval[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T2Ge8GbDb9K",
        "outputId": "c7aa54de-78f2-48f0-f171-3231b3714ae8"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2826, 2252],\n",
              "       [2826, 2722],\n",
              "       [2252, 2826],\n",
              "       [2252, 2722],\n",
              "       [2252, 2826],\n",
              "       [2722, 2826],\n",
              "       [2722, 2252],\n",
              "       [2722, 2826],\n",
              "       [2722, 2826],\n",
              "       [2826, 2252],\n",
              "       [2826, 2722],\n",
              "       [2826, 2826],\n",
              "       [2826, 3634],\n",
              "       [2826, 2722],\n",
              "       [2826, 2826],\n",
              "       [2826, 3634],\n",
              "       [2826, 2826],\n",
              "       [3634, 2826],\n",
              "       [3634, 2826],\n",
              "       [3634, 2826]])"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_layer(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x"
      ],
      "metadata": {
        "id": "0mOIRF2B30BL"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 5\n",
        "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
        "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "pItKrMJVzck4"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, W1, W2):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.W1 = W1\n",
        "        self.W2 = W2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1)\n",
        "        z1 = torch.matmul(self.W1, x)\n",
        "        z2 = torch.matmul(self.W2, z1)\n",
        "        return z2"
      ],
      "metadata": {
        "id": "ViYyOUq7-4ta"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(idx_pairs, embedding_dims, vocabulary_size, num_epochs=100, learning_rate=0.001):\n",
        "    W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
        "    W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
        "\n",
        "    for epo in range(num_epochs):\n",
        "        loss_val = 0\n",
        "        for data, target in idx_pairs:\n",
        "          x = Variable(get_input_layer(data)).float()\n",
        "          y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "          z1 = torch.matmul(W1, x)\n",
        "          z2 = torch.matmul(W2, z1)\n",
        "\n",
        "          log_softmax = F.log_softmax(z2, dim=0)\n",
        "\n",
        "          loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "          loss_val += loss.data\n",
        "          loss.backward()\n",
        "          W1.data -= learning_rate * W1.grad.data\n",
        "          W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "          W1.grad.data.zero_()\n",
        "          W2.grad.data.zero_()\n",
        "        if epo % 5 == 0:\n",
        "            print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
        "    model = SimpleModel(W1, W2)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JKneDOue3q6Z"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def compute_perplexity(model, test_data):\n",
        "    idx_pairs_text_data = get_idx_pairs(test_data, word2idx)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.\n",
        "    for data, target in idx_pairs_text_data:\n",
        "        x = Variable(get_input_layer(data)).float()\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "        z1 = torch.matmul(model.W1, x)\n",
        "        z2 = torch.matmul(model.W2, z1)\n",
        "\n",
        "        log_softmax = torch.nn.functional.log_softmax(z2, dim=0)\n",
        "        loss = criterion(log_softmax.view(1,-1), y_true)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    mean_loss = total_loss / len(idx_pairs_text_data)\n",
        "    perplexity = math.exp(mean_loss)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "QdLmTWjd_BZi"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 20\n",
        "num_epochs = 30\n",
        "learning_rate = 0.01\n",
        "model = train_network(idx_pairs, embedding_dims, vocabulary_size, num_epochs, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROF_nTCx2QBA",
        "outputId": "6df4918a-8c95-4c48-d82e-001ab573d451"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epo 0: 15.058667182922363\n",
            "Loss at epo 5: 10.217268943786621\n",
            "Loss at epo 10: 8.54259967803955\n",
            "Loss at epo 15: 7.572190761566162\n",
            "Loss at epo 20: 6.9180073738098145\n",
            "Loss at epo 25: 6.429164409637451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = compute_perplexity(model, filtered_validation_sentences)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Qw7mn4cyNg",
        "outputId": "09d17363-95dd-4cbe-ae22-825b5422518e"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39915.24910830672\n"
          ]
        }
      ]
    }
  ]
}