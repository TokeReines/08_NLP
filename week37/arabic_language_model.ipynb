{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wA5vsqOk_0RT",
    "outputId": "e8845d25-649b-4105-bc7d-1a0685220159"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/duy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/duy/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "arabic_df = pd.read_feather('./arabic_train_set.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YqRcZbIHJikX"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# as per recommendation from @freylis, compile once only\n",
    "CLEANR = re.compile('<.*?>')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FHwnR32qKZHZ"
   },
   "outputs": [],
   "source": [
    "def break_into_sentences(paragraph):\n",
    "    sentences = list()\n",
    "    temp_sentence = list()\n",
    "    flag = False\n",
    "    for ch in paragraph.strip():\n",
    "        if ch in [u'؟', u'!', u'.', u':', u'؛']:\n",
    "            flag = True\n",
    "        elif flag:\n",
    "            sentences.append(''.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "            flag = False\n",
    "\n",
    "        temp_sentence.append(ch)\n",
    "\n",
    "    else:\n",
    "        sentences.append(''.join(temp_sentence).strip())\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bLhdlLQcL3V8"
   },
   "outputs": [],
   "source": [
    "def remove_ref(sentence):\n",
    "  result = re.sub(\"(\\[\\d\\])\", \"\", sentence)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_0pWdMsoHyGn"
   },
   "outputs": [],
   "source": [
    "def clean_arabic(l_arabic):\n",
    "  l_cleaned_arabic = []\n",
    "  for p in l_arabic:\n",
    "    ss = break_into_sentences(remove_ref(cleanhtml(p)))\n",
    "    for s in ss:\n",
    "      l_cleaned_arabic.append(s)\n",
    "  return l_cleaned_arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████████████████████| 4.94k/4.94k [00:00<00:00, 16.5MB/s]\n",
      "Downloading metadata: 100%|████████████████████████████████████| 2.47k/2.47k [00:00<00:00, 7.11MB/s]\n",
      "Downloading data files:   0%|                                                 | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                 | 0.00/71.6M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   6%|██▎                                     | 4.19M/71.6M [00:01<00:24, 2.70MB/s]\u001b[A\n",
      "Downloading data:  18%|███████                                 | 12.6M/71.6M [00:02<00:12, 4.73MB/s]\u001b[A\n",
      "Downloading data:  29%|███████████▋                            | 21.0M/71.6M [00:04<00:09, 5.12MB/s]\u001b[A\n",
      "Downloading data:  41%|████████████████▍                       | 29.4M/71.6M [00:05<00:07, 6.03MB/s]\u001b[A\n",
      "Downloading data:  53%|█████████████████████                   | 37.7M/71.6M [00:06<00:05, 6.63MB/s]\u001b[A\n",
      "Downloading data:  64%|█████████████████████████▊              | 46.1M/71.6M [00:07<00:03, 6.84MB/s]\u001b[A\n",
      "Downloading data:  76%|██████████████████████████████▍         | 54.5M/71.6M [00:08<00:02, 6.86MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████████████████████▏    | 62.9M/71.6M [00:09<00:01, 7.20MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████| 71.6M/71.6M [00:11<00:00, 6.28MB/s]\u001b[A\n",
      "Downloading data files:  50%|████████████████████▌                    | 1/2 [00:11<00:11, 11.40s/it]\n",
      "Downloading data:   0%|                                                 | 0.00/7.49M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████| 7.49M/7.49M [00:02<00:00, 3.51MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████████████████████████| 2/2 [00:13<00:00,  6.77s/it]\n",
      "Extracting data files: 100%|████████████████████████████████████████| 2/2 [00:00<00:00, 5029.14it/s]\n",
      "Generating train split: 100%|████████████████████| 116067/116067 [00:00<00:00, 605358.57 examples/s]\n",
      "Generating validation split: 100%|█████████████████| 13325/13325 [00:00<00:00, 703096.00 examples/s]\n",
      "Filter: 100%|██████████████████████████████████████| 13325/13325 [00:00<00:00, 161111.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "valid_set = dataset[\"validation\"].filter(lambda example, idx: example['language'] == 'arabic', with_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M4zxLPvCLJlF"
   },
   "outputs": [],
   "source": [
    "l_cleaned_arabic_train = clean_arabic(list(arabic_df['document_plaintext']))\n",
    "l_cleaned_arabic_val = clean_arabic(list(valid_set['document_plaintext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qRzgmleg3ZZB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "UNK = '<unk>'\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.idx2word = {}\n",
    "        self.word2idx = defaultdict(lambda: 0)\n",
    "        self.idx = 0\n",
    "        self.idx2word[0] = UNK\n",
    "        self.word2idx[UNK] = 0\n",
    "        self.idx = 1\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, l_arabic, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        # with open(path, 'r') as f:\n",
    "        #     tokens = 0\n",
    "        #     for line in f:\n",
    "        #         words = line.split() + ['<eos>']\n",
    "        #         tokens += len(words)\n",
    "        #         for word in words:\n",
    "        #             self.dictionary.add_word(word)\n",
    "\n",
    "        tokens = 0\n",
    "        for l in l_arabic:\n",
    "          words = word_tokenize(l) + ['<eos>']\n",
    "          tokens += len(words)\n",
    "          for word in words:\n",
    "            self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        # with open(path, 'r') as f:\n",
    "        #     for line in f:\n",
    "        #         words = line.split() + ['<eos>']\n",
    "        #         for word in words:\n",
    "        #             ids[token] = self.dictionary.word2idx[word]\n",
    "        #             token += 1\n",
    "        for l in l_arabic:\n",
    "          words = word_tokenize(l) + ['<eos>']\n",
    "          for word in words:\n",
    "            ids[token] = self.dictionary.word2idx[word]\n",
    "            token += 1\n",
    "\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1qQhQQq7Nqz",
    "outputId": "fe6fb45e-4ef3-400a-8a4c-da4bbe58b4dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 200     # number of words to be sampled\n",
    "batch_size = 45\n",
    "seq_length = 30\n",
    "learning_rate = 2e-4\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsOArMC08SEQ",
    "outputId": "7e2d8cde-3d51-45dc-912e-28803cb75edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "ids = corpus.get_data(l_cleaned_arabic_train, batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "test_corpus = Corpus()\n",
    "test_ids = test_corpus.get_data(l_cleaned_arabic_val, batch_size)\n",
    "test_vocab_size =  len(test_corpus.dictionary)\n",
    "test_num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "print(num_batches)\n",
    "# print(test_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "az1yCyYj8d5i"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "\n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "\n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)\n",
    "\n",
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZ38A6fo84u1",
    "outputId": "9bfde073-22de-4aa5-c067-6d5348814fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/2014], Loss: 12.3197, Perplexity: 224057.99\n",
      "Epoch [1/5], Step[500/2014], Loss: 8.0985, Perplexity: 3289.48\n",
      "Epoch [1/5], Step[1000/2014], Loss: 8.0992, Perplexity: 3291.81\n",
      "Epoch [1/5], Step[1500/2014], Loss: 7.7678, Perplexity: 2363.30\n",
      "Epoch [1/5], Step[2000/2014], Loss: 7.8666, Perplexity: 2608.75\n",
      "Epoch [2/5], Step[0/2014], Loss: 7.7353, Perplexity: 2287.74\n",
      "Epoch [2/5], Step[500/2014], Loss: 6.6503, Perplexity: 773.01\n",
      "Epoch [2/5], Step[1000/2014], Loss: 6.9875, Perplexity: 1082.96\n",
      "Epoch [2/5], Step[1500/2014], Loss: 6.6587, Perplexity: 779.52\n",
      "Epoch [2/5], Step[2000/2014], Loss: 6.8869, Perplexity: 979.41\n",
      "Epoch [3/5], Step[0/2014], Loss: 6.8098, Perplexity: 906.67\n",
      "Epoch [3/5], Step[500/2014], Loss: 5.6924, Perplexity: 296.61\n",
      "Epoch [3/5], Step[1000/2014], Loss: 6.1823, Perplexity: 484.11\n",
      "Epoch [3/5], Step[1500/2014], Loss: 5.8611, Perplexity: 351.09\n",
      "Epoch [3/5], Step[2000/2014], Loss: 6.0567, Perplexity: 426.97\n",
      "Epoch [4/5], Step[0/2014], Loss: 6.0479, Perplexity: 423.21\n",
      "Epoch [4/5], Step[500/2014], Loss: 4.9680, Perplexity: 143.74\n",
      "Epoch [4/5], Step[1000/2014], Loss: 5.4989, Perplexity: 244.43\n",
      "Epoch [4/5], Step[1500/2014], Loss: 5.2508, Perplexity: 190.71\n",
      "Epoch [4/5], Step[2000/2014], Loss: 5.3826, Perplexity: 217.59\n",
      "Epoch [5/5], Step[0/2014], Loss: 5.3955, Perplexity: 220.41\n",
      "Epoch [5/5], Step[500/2014], Loss: 4.4197, Perplexity: 83.07\n",
      "Epoch [5/5], Step[1000/2014], Loss: 4.9399, Perplexity: 139.75\n",
      "Epoch [5/5], Step[1500/2014], Loss: 4.7250, Perplexity: 112.73\n",
      "Epoch [5/5], Step[2000/2014], Loss: 4.8325, Perplexity: 125.52\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        # print('input', inputs, 'output', targets)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        states = detach(states)\n",
    "        # print(inputs.shape)\n",
    "        # print(states[0].shape)\n",
    "        # print(targets.shape)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 500 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "XXvugrfqT52a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './arabic_document.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./arabic_document.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "test loss 12.04 | test ppl 168735.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "test_loss = 0.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        test_loss += criterion(outputs, targets.reshape(-1)).item()\n",
    "\n",
    "test_loss = test_loss / test_num_batches\n",
    "print('-' * 89)\n",
    "print('test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('-' * 89)\n",
    "\n",
    "\n",
    "# Generate texts using trained model\n",
    "# with torch.no_grad():\n",
    "#     with open('sample.txt', 'w') as f:\n",
    "#         # Set intial hidden ane cell states\n",
    "#         state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "#                  torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "#         # Select one word id randomly\n",
    "#         prob = torch.ones(vocab_size)\n",
    "#         input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "#         for i in range(num_samples):\n",
    "#             # Forward propagate RNN \n",
    "#             output, state = model(input, state)\n",
    "\n",
    "#             # Sample a word id\n",
    "#             prob = output.exp()\n",
    "#             word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "#             # Fill input with sampled word id for the next time step\n",
    "#             input.fill_(word_id)\n",
    "\n",
    "#             # File write\n",
    "#             word = corpus.dictionary.idx2word[word_id]\n",
    "#             word = '\\n' if word == '<eos>' else word + ' '\n",
    "#             f.write(word)\n",
    "\n",
    "#             if (i+1) % 100 == 0:\n",
    "#                 print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
