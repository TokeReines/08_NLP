{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from bpemb import BPEmb\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "# Define the languages of interest\n",
    "languages = [\"arabic\", \"bengali\", \"indonesian\"]\n",
    "language_to_bpe = {'bengali': BPEmb(lang=\"bn\", dim=50), 'indonesian': BPEmb(lang=\"id\", dim=50), 'arabic': BPEmb(lang=\"ar\", dim=50)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd544f089f94f5ea90264de5d62cd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/116067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_set.filter(lambda example: example['language'] == 'indonesian' and example['document_plaintext'] is not None)[0]\n",
    "sample_row['document_plaintext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing using BPEmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokens: ['▁ern', 'est', '▁d', 'ou', 'w', 'es', '▁dek', 'ker', '▁wafat', '▁dini']\n"
     ]
    }
   ],
   "source": [
    "bpemb_model = language_to_bpe[sample_row['language']]\n",
    "\n",
    "question_bpe = bpemb_model.encode(sample_row['question_text'])\n",
    "document_bpe = bpemb_model.encode(sample_row['document_plaintext'])\n",
    "print(\"BPE tokens:\", document_bpe[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 tokens: ['Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "\n",
    "question_gpt2 = tokenizer.tokenize(sample_row['question_text'])\n",
    "document_gpt2 = tokenizer.tokenize(sample_row['document_plaintext'])\n",
    "print(\"GPT2 tokens:\", document_gpt2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Er', 'nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "n = 2\n",
    "\n",
    "padded_text = list(pad_both_ends(document_gpt2, n=n))\n",
    "print(padded_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'Er'), ('Er', 'nest'), ('nest', 'ĠD'), ('ĠD', 'ouw'), ('ouw', 'es'), ('es', 'ĠDek'), ('ĠDek', 'ker'), ('ker', 'Ġwafat'), ('Ġwafat', 'Ġdini'), ('Ġdini', 'Ġhari'), ('Ġhari', 'Ġtanggal'), ('Ġtanggal', 'Ġ28'), ('Ġ28', 'ĠAgustus'), ('ĠAgustus', 'Ġ1950'), ('Ġ1950', 'Ġ('), ('Ġ(', 'ter'), ('ter', 'tulis'), ('tulis', 'Ġdi'), ('Ġdi', 'Ġbatu'), ('Ġbatu', 'Ġnis'), ('Ġnis', 'annya'), ('annya', ';'), (';', 'Ġ29'), ('Ġ29', 'ĠAgustus'), ('ĠAgustus', 'Ġ1950'), ('Ġ1950', 'Ġversi'), ('Ġversi', 'Ġvan'), ('Ġvan', 'Ġder'), ('Ġder', 'ĠV'), ('ĠV', 'eur'), ('eur', ','), (',', 'Ġ2006'), ('Ġ2006', ')'), (')', 'Ġdan'), ('Ġdan', 'Ġdimakamkan'), ('Ġdimakamkan', 'Ġdi'), ('Ġdi', 'ĠT'), ('ĠT', 'MP'), ('MP', 'ĠC'), ('ĠC', 'ikut'), ('ikut', 'ra'), ('ra', ','), (',', 'ĠBandung'), ('ĠBandung', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "padded_n_grams = ngrams(padded_text, n=n)\n",
    "print(list(padded_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nest', 'ĠD', 'ouw', 'es', 'ĠDek', 'ker', 'Ġwafat', 'Ġdini', 'Ġhari', 'Ġtanggal', 'Ġ28', 'ĠAgustus', 'Ġ1950', 'Ġversi', 'Ġvan', 'Ġder', 'ĠV', 'eur', ',', 'Ġ2006']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [document_gpt2])\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "language = 'indonesian'\n",
    "n = 2\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('indonesian-nlp/gpt2-medium-indonesian')\n",
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in train_set if row['language'] == language))\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, [gpt2_tokens])\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'Kol')\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokens = tokenizer.tokenize(\" \".join(row['document_plaintext'] for row in validation_set if row['language'] == language))\n",
    "# validation_data, _ = padded_everygram_pipeline(n, [gpt2_tokens])\n",
    "padded_tokens_validation = list(pad_sequence(gpt2_tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=n))\n",
    "perplexity = model.perplexity(bigrams(padded_tokens_validation))\n",
    "# Calculate perplexity\n",
    "print(list(bigrams(padded_tokens_validation))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
