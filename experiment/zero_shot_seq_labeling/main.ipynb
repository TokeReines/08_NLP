{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: bengali\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d451d0b3be94de38a4953b181bf7141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600b9f3b0544cb7acaa087e41e844ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6437f6dbc4b9aa24ebd9f7f7307f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44c6fe59d24c599e576f651931ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c27d717b5f4b68b8b52ca3d8fe24ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7069f8d827cf4e2884333867151ca8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1486/1486 [03:21<00:00,  7.38it/s]\n",
      "100%|██████████| 1486/1486 [04:15<00:00,  5.83it/s]\n",
      "100%|██████████| 1486/1486 [04:17<00:00,  5.77it/s]\n",
      "Evaluation: 100%|██████████| 9/9 [00:02<00:00,  3.27it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1121.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 28.125, 'f1': 34.021687637759065, 'total': 224, 'HasAns_exact': 28.125, 'HasAns_f1': 34.021687637759065, 'HasAns_total': 224, 'best_exact': 28.125, 'best_exact_thresh': 0.0, 'best_f1': 34.021687637759065, 'best_f1_thresh': 0.0}\n",
      "Language: english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0978523af7849509d9f5288fd8cbadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66cffccb554ef1b4b2737cfb9803ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2013/2013 [05:12<00:00,  6.45it/s]\n",
      "100%|██████████| 2013/2013 [05:11<00:00,  6.47it/s]\n",
      "100%|██████████| 2013/2013 [05:11<00:00,  6.47it/s]\n",
      "Evaluation: 100%|██████████| 34/34 [00:11<00:00,  2.91it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1240.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 25.95959595959596, 'f1': 32.8463579671379, 'total': 990, 'HasAns_exact': 25.95959595959596, 'HasAns_f1': 32.8463579671379, 'HasAns_total': 990, 'best_exact': 25.95959595959596, 'best_exact_thresh': 0.0, 'best_f1': 32.8463579671379, 'best_f1_thresh': 0.0}\n",
      "Language: indonesian\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e984563dbfc4bbe9b154a413e54a8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe569c5e50b43399ecc4132b5da6f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3003/3003 [07:01<00:00,  7.12it/s]\n",
      "100%|██████████| 3003/3003 [06:59<00:00,  7.15it/s]\n",
      "100%|██████████| 3003/3003 [07:06<00:00,  7.05it/s]\n",
      "Evaluation: 100%|██████████| 40/40 [00:12<00:00,  3.10it/s]\n",
      "100%|██████████| 1191/1191 [00:00<00:00, 1317.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 34.17296389588581, 'f1': 39.967987795222456, 'total': 1191, 'HasAns_exact': 34.17296389588581, 'HasAns_f1': 39.967987795222456, 'HasAns_total': 1191, 'best_exact': 34.17296389588581, 'best_exact_thresh': 0.0, 'best_f1': 39.967987795222456, 'best_f1_thresh': 0.0}\n",
      "Language: arabic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323caba737054834a661b7b260384642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affadaf1c5ea4190abf50d3738343311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 8443/8443 [22:24<00:00,  6.28it/s]\n",
      "100%|██████████| 8443/8443 [22:14<00:00,  6.33it/s]\n",
      "100%|██████████| 8443/8443 [22:17<00:00,  6.31it/s]\n",
      "Evaluation: 100%|██████████| 66/66 [00:23<00:00,  2.84it/s]\n",
      "100%|██████████| 1902/1902 [00:01<00:00, 1219.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 30.336487907465827, 'f1': 38.44675862942356, 'total': 1902, 'HasAns_exact': 30.336487907465827, 'HasAns_f1': 38.44675862942356, 'HasAns_total': 1902, 'best_exact': 30.336487907465827, 'best_exact_thresh': 0.0, 'best_f1': 38.44675862942356, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from helper import collate_fn, get_train_features, get_validation_features, post_process_predictions, predict, val_collate_fn\n",
    "from lab6_train import *\n",
    "\n",
    "lr = 2e-5\n",
    "n_epochs = 3\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 200\n",
    "bert_map = {\n",
    "    'bengali': 'xlm-roberta-base', \n",
    "    'english': 'xlm-roberta-base', \n",
    "    'indonesian': 'xlm-roberta-base', \n",
    "    'arabic': 'xlm-roberta-base'\n",
    "}\n",
    "device = 'cuda'\n",
    "\n",
    "compute_squad = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].add_column('id', list(range(len(dataset[split]))))\n",
    "\n",
    "for language, bert in list(bert_map.items()):\n",
    "    print(f'Language: {language}')\n",
    "    language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "    tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "    tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "    tokenized_validation_dataset = language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names)\n",
    "\n",
    "    train_dl = DataLoader(tokenized_train_dataset, collate_fn=collate_fn, shuffle=True, batch_size=4)\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(bert).to(device)\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        n_epochs * len(train_dl)\n",
    "    )\n",
    "\n",
    "    losses = train(\n",
    "        model,\n",
    "        train_dl,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        n_epochs,\n",
    "        device\n",
    "    )\n",
    "    torch.save(model, f'{language}_xlm-roberta-base_span_detection.pt')\n",
    "    model = torch.load(f'{language}_xlm-roberta-base_span_detection.pt')\n",
    "    for language, bert in list(bert_map.items()):\n",
    "        language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "        tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "        tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "        tokenized_validation_dataset = language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names)\n",
    "\n",
    "        train_dl = DataLoader(tokenized_train_dataset, collate_fn=collate_fn, shuffle=True, batch_size=4)\n",
    "        val_dl = DataLoader(tokenized_validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n",
    "        \n",
    "        logits = predict(model, val_dl, device)\n",
    "        predictions = post_process_predictions(language_dataset['validation'], tokenized_validation_dataset, logits)\n",
    "        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.} for k, v in predictions.items()]\n",
    "        gold = [{\n",
    "            'id': example['id'],\n",
    "            'answers': {\n",
    "                'text': example['annotations']['answer_text'],\n",
    "                'answer_start': example['annotations']['answer_start']}\n",
    "            }\n",
    "            for example in language_dataset['validation']]\n",
    "\n",
    "\n",
    "        print(compute_squad.compute(references=gold, predictions=formatted_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: bengali\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 9/9 [00:01<00:00,  5.39it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1118.94it/s]\n",
      "Evaluation: 100%|██████████| 34/34 [00:06<00:00,  5.10it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1250.43it/s]\n",
      "Evaluation: 100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n",
      "100%|██████████| 1191/1191 [00:00<00:00, 1339.36it/s]\n",
      "Evaluation: 100%|██████████| 66/66 [00:15<00:00,  4.23it/s]\n",
      "100%|██████████| 1902/1902 [00:01<00:00, 1184.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 9/9 [00:01<00:00,  5.32it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1132.63it/s]\n",
      "Evaluation: 100%|██████████| 34/34 [00:08<00:00,  4.20it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1249.72it/s]\n",
      "Evaluation: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "100%|██████████| 1191/1191 [00:00<00:00, 1326.67it/s]\n",
      "Evaluation: 100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n",
      "100%|██████████| 1902/1902 [00:01<00:00, 1232.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: indonesian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 9/9 [00:01<00:00,  5.18it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1122.07it/s]\n",
      "Evaluation: 100%|██████████| 34/34 [00:09<00:00,  3.75it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1231.18it/s]\n",
      "Evaluation: 100%|██████████| 40/40 [00:10<00:00,  3.95it/s]\n",
      "100%|██████████| 1191/1191 [00:00<00:00, 1332.25it/s]\n",
      "Evaluation: 100%|██████████| 66/66 [00:19<00:00,  3.41it/s]\n",
      "100%|██████████| 1902/1902 [00:01<00:00, 1224.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: arabic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 9/9 [00:01<00:00,  4.75it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1112.89it/s]\n",
      "Evaluation: 100%|██████████| 34/34 [00:09<00:00,  3.44it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1234.54it/s]\n",
      "Evaluation: 100%|██████████| 40/40 [00:10<00:00,  3.66it/s]\n",
      "100%|██████████| 1191/1191 [00:00<00:00, 1310.22it/s]\n",
      "Evaluation: 100%|██████████| 66/66 [00:20<00:00,  3.23it/s]\n",
      "100%|██████████| 1902/1902 [00:01<00:00, 1226.49it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = {l: {} for l in bert_map.keys()}\n",
    "for language, bert in list(bert_map.items()):\n",
    "    print(f'Language: {language}')\n",
    "    model = torch.load(f'{language}_xlm-roberta-base_span_detection.pt')\n",
    "    for language2, bert in list(bert_map.items()):\n",
    "        language_dataset = dataset.filter(lambda example: example['language'] == language2)\n",
    "        tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "        tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "        tokenized_validation_dataset = language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names)\n",
    "\n",
    "        train_dl = DataLoader(tokenized_train_dataset, collate_fn=collate_fn, shuffle=True, batch_size=4)\n",
    "        val_dl = DataLoader(tokenized_validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n",
    "        \n",
    "        logits = predict(model, val_dl, device)\n",
    "        predictions = post_process_predictions(language_dataset['validation'], tokenized_validation_dataset, logits)\n",
    "        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.} for k, v in predictions.items()]\n",
    "        gold = [{\n",
    "            'id': example['id'],\n",
    "            'answers': {\n",
    "                'text': example['annotations']['answer_text'],\n",
    "                'answer_start': example['annotations']['answer_start']}\n",
    "            }\n",
    "            for example in language_dataset['validation']]\n",
    "\n",
    "\n",
    "        metrics[language][language2] = compute_squad.compute(references=gold, predictions=formatted_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match scores, xlm-r span detection:\n",
      "tuned lan & bengali & english & indonesian & arabic\\\\\n",
      "bengali & 28.12 & 21.31 & 20.49 & 13.72\\\\\n",
      "english & 15.62 & 25.96 & 27.54 & 20.08\\\\\n",
      "indonesian & 17.86 & 26.16 & 34.17 & 25.13\\\\\n",
      "arabic & 18.30 & 26.16 & 29.89 & 30.34\\\\\n",
      "Average & 20.91 & 22.30 & 25.83 & 26.17\\\\\n"
     ]
    }
   ],
   "source": [
    "# Pretty print metrics\n",
    "print(\"Exact match scores, xlm-r span detection:\")\n",
    "print(' & '.join(['tuned lan'] + list(bert_map.keys())) + '\\\\\\\\')\n",
    "for language in bert_map.keys():\n",
    "    print(' & '.join([language] + [f'{metrics[language][language2][\"exact\"]:.2f}' for language2 in bert_map.keys()]) + '\\\\\\\\')\n",
    "print(' & '.join(['Average'] + [f'{sum([metrics[language][language2][\"exact\"] for language2 in bert_map.keys()]) / len(bert_map.keys()):.2f}' for language in bert_map.keys()]) + '\\\\\\\\')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSPSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
