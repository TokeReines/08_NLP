{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "bert_map = {    \n",
    "    'arabic': 'arabic_cls_bert_logreg_classifier.pt',\n",
    "    'bengali': 'google/muril-base-cased', \n",
    "    'english': 'bert-base-uncased', \n",
    "    'indonesian': 'cahya/bert-base-indonesian-522M',\n",
    "}\n",
    "device = 'cuda'\n",
    "\n",
    "compute_squad = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].add_column('id', list(range(len(dataset[split]))))\n",
    "\n",
    "language = 'english'\n",
    "n = 15\n",
    "stride = 10\n",
    "max_length = 30\n",
    "model = torch.load(f'{language}_xlm-roberta-base_classification.pt')\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(bert_map[language]).to(device)\n",
    "language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "tk = AutoTokenizer.from_pretrained('xlm-roberta-base', max_len=300, use_fast=True)\n",
    "\n",
    "questions = language_dataset['validation'][200:500]['question_text']\n",
    "documents = language_dataset['validation'][200:500]['document_plaintext']\n",
    "annotations = language_dataset['validation'][200:500]['annotations']\n",
    "# Iterate over each question-document pair\n",
    "for i, (question, document, annotation) in enumerate(zip(questions, documents, annotations)):\n",
    "    # Tokenize the input and send to CUDA )\n",
    "    inputs = tk(question, document, return_tensors=\"pt\", truncation=\"only_second\", padding=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n",
    "    #inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    overflow_to_sample_mapping = inputs.overflow_to_sample_mapping.to(device)\n",
    "    \n",
    "    # Forward pass, get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs_span = model(input_ids, attention_mask=attention_mask, return_dict=True, output_attentions=True)\n",
    "        # outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Find the best answer among the chunks\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "    answer_start = torch.argmax(torch.from_numpy(start_logits), dim=1).cpu()[0]\n",
    "    answer_end = torch.argmax(torch.from_numpy(end_logits), dim=1).cpu()[0]\n",
    "    scores = start_logits + end_logits\n",
    "    best_chunk = torch.argmax(torch.from_numpy(scores).max(dim=1).values).item()\n",
    "\n",
    "    # Convert token ids to tokens\n",
    "    tokens = tk.convert_ids_to_tokens(input_ids[best_chunk])\n",
    "    # Adjust answer_start and answer_end for long documents\n",
    "    if best_chunk > 0:\n",
    "        chunk_offset = stride * best_chunk\n",
    "        answer_start += chunk_offset\n",
    "        answer_end += chunk_offset\n",
    "    \n",
    "    answer_start = answer_start.item()    \n",
    "    answer_end = answer_end.item()\n",
    "    \n",
    "    \n",
    "    answer_tokens = tokens[answer_start: answer_end + 1]\n",
    "    predicted_answer = tk.convert_tokens_to_string(answer_tokens)\n",
    "    \n",
    "    print(\"Predicted answer:\", predicted_answer)\n",
    "    print(\"True answer:\", annotation['answer_text'][0])\n",
    "    if predicted_answer.lower().strip() != annotation['answer_text'][0].lower().strip():\n",
    "        continue\n",
    "        \n",
    "    attention_weights = outputs.attentions[-1]  # Get the last layer's attention weights\n",
    "\n",
    "    # Select the best chunk\n",
    "    chunk_attention = attention_weights[best_chunk]\n",
    "\n",
    "    # Convert token ids to tokens for the entire sequence in the best chunk\n",
    "    tokens = tk.convert_ids_to_tokens(input_ids[best_chunk].cpu().numpy())\n",
    "\n",
    "    # Identify the position of the [SEP] token\n",
    "    sep_pos = tokens.index('</s>')\n",
    "\n",
    "    # Now, select the attention weights for the question tokens and the answer span\n",
    "    # The shape of combined_attention will be (num_heads, combined_length, sequence_length)\n",
    "    combined_attention = chunk_attention[:, :sep_pos+answer_end+2, :]\n",
    "\n",
    "    # Visualize the attention weights\n",
    "    for head in range(combined_attention.shape[0]):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(combined_attention[head].cpu().numpy(), xticklabels=tokens, yticklabels=tokens[:sep_pos+answer_end+2], cmap='Blues', vmin=0, vmax=1)\n",
    "        plt.title(f'Head {head+1}, Question and Answer Span Attention')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.show()\n",
    "        \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
