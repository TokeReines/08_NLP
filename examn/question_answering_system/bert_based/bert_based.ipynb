{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from helper import collate_fn, get_train_features, get_validation_features, post_process_predictions, predict, val_collate_fn\n",
    "from lab6_train import *\n",
    "\n",
    "lr = 2e-5\n",
    "n_epochs = 3\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 200\n",
    "bert_map = {\n",
    "    'bengali': 'google/muril-base-cased', \n",
    "    'english': 'bert-base-uncased', \n",
    "    'indonesian': 'cahya/bert-base-indonesian-522M', \n",
    "    'arabic': 'asafaya/bert-base-arabic'\n",
    "}\n",
    "device = 'cuda'\n",
    "\n",
    "compute_squad = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].add_column('id', list(range(len(dataset[split]))))\n",
    "\n",
    "for language, bert in list(bert_map.items()):\n",
    "    print(f'Language: {language}')\n",
    "    language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "    tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "    tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "    tokenized_validation_dataset = language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names)\n",
    "\n",
    "    train_dl = DataLoader(tokenized_train_dataset, collate_fn=collate_fn, shuffle=True, batch_size=8)\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(bert).to(device)\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        n_epochs * len(train_dl)\n",
    "    )\n",
    "\n",
    "    losses = train(\n",
    "        model,\n",
    "        train_dl,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        n_epochs,\n",
    "        device\n",
    "    )\n",
    "    torch.save(model, f'{language}_span_detection.pt')\n",
    "    \n",
    "# for language, bert in list(bert_map.items()):\n",
    "#     print(f'Language: {language}')\n",
    "#     model = torch.load(f'{language}_span_detection.pt')\n",
    "#     language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "#     tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "#     tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "#     tokenized_validation_dataset = language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names)\n",
    "\n",
    "#     val_dl = DataLoader(tokenized_validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n",
    "    \n",
    "#     logits = predict(model, val_dl, device)\n",
    "#     predictions = post_process_predictions(language_dataset['validation'], tokenized_validation_dataset, logits)\n",
    "#     formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.} for k, v in predictions.items()]\n",
    "#     gold = [{\n",
    "#         'id': example['id'],\n",
    "#         'answers': {\n",
    "#             'text': example['annotations']['answer_text'],\n",
    "#             'answer_start': example['annotations']['answer_start']}\n",
    "#         }\n",
    "#         for example in language_dataset['validation']]\n",
    "\n",
    "\n",
    "#     print(compute_squad.compute(references=gold, predictions=formatted_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
