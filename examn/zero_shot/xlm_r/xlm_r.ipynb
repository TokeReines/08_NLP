{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the XLM-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from helper import collate_fn, get_train_features, get_validation_features, post_process_predictions, predict, val_collate_fn\n",
    "from lab6_train import *\n",
    "\n",
    "lr = 2e-5\n",
    "n_epochs = 3\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 200\n",
    "bert_map = {\n",
    "    'bengali': 'xlm-roberta-base',\n",
    "    'english': 'xlm-roberta-base',\n",
    "    'indonesian': 'xlm-roberta-base',\n",
    "    'arabic': 'xlm-roberta-base'\n",
    "}\n",
    "device = 'cuda'\n",
    "\n",
    "compute_squad = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].add_column('id', list(range(len(dataset[split]))))\n",
    "\n",
    "for language, bert in list(bert_map.items()):\n",
    "    print(f'Language: {language}')\n",
    "    language_dataset = dataset.filter(lambda example: example['language'] == language)\n",
    "    tk = AutoTokenizer.from_pretrained(bert, max_len=300)\n",
    "\n",
    "    tokenized_train_dataset = language_dataset['train'].map(partial(get_train_features, tk), batched=True, remove_columns=language_dataset['train'].column_names)\n",
    "\n",
    "    train_dl = DataLoader(tokenized_train_dataset, collate_fn=collate_fn, shuffle=True, batch_size=8)\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(bert).to(device)\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        n_epochs * len(train_dl)\n",
    "    )\n",
    "\n",
    "    losses = train(\n",
    "        model,\n",
    "        train_dl,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        n_epochs,\n",
    "        device\n",
    "    )\n",
    "    torch.save(model, f'{language}_xml_roberta_base_span_detection_2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Zero-shot multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tk = AutoTokenizer.from_pretrained('xlm-roberta-base', max_len=300)\n",
    "bert_map = {\n",
    "    'bengali': 'xlm-roberta-base',\n",
    "    'english': 'xlm-roberta-base',\n",
    "    'indonesian': 'xlm-roberta-base',\n",
    "    'arabic': 'xlm-roberta-base'\n",
    "}\n",
    "metrics = {l: {} for l in bert_map.keys()}\n",
    "datasets = {\n",
    "    l: dataset.filter(lambda example: example['language'] == l) for l in bert_map.keys()\n",
    "}\n",
    "tokenized_validation_datasets = {\n",
    "    l: language_dataset['validation'].map(partial(get_validation_features, tk), batched=True, remove_columns=language_dataset['validation'].column_names) for l, language_dataset in datasets.items()   \n",
    "}\n",
    "for language, bert in list(bert_map.items()):\n",
    "    print(f'Language: {language}')\n",
    "    model = torch.load(f'{language}_xml_roberta_base_span_detection_2.pt')\n",
    "    for language2, bert in list(bert_map.items()):\n",
    "        language_dataset = datasets[language2]\n",
    "\n",
    "        tokenized_validation_dataset = tokenized_validation_datasets[language2] \n",
    "\n",
    "        val_dl = DataLoader(tokenized_validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n",
    "        \n",
    "        logits = predict(model, val_dl, device)\n",
    "        predictions = post_process_predictions(language_dataset['validation'], tokenized_validation_dataset, logits)\n",
    "        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.} for k, v in predictions.items()]\n",
    "        gold = [{\n",
    "            'id': example['id'],\n",
    "            'answers': {\n",
    "                'text': example['annotations']['answer_text'],\n",
    "                'answer_start': example['annotations']['answer_start']}\n",
    "            }\n",
    "            for example in language_dataset['validation']]\n",
    "\n",
    "\n",
    "        metrics[language][language2] = compute_squad.compute(references=gold, predictions=formatted_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print EM\n",
    "print(\"Exact match scores, xlm-r span detection:\")\n",
    "print(' & '.join(['tuned lan'] + list(bert_map.keys())) + '\\\\\\\\')\n",
    "for language in bert_map.keys():\n",
    "    print(' & '.join([language] + [f'{metrics[language][language2][\"exact\"]:.2f}' for language2 in bert_map.keys()]) + '\\\\\\\\')\n",
    "print(' & '.join(['Average'] + [f'{sum([metrics[language][language2][\"exact\"] for language2 in bert_map.keys()]) / len(bert_map.keys()):.2f}' for language in bert_map.keys()]) + '\\\\\\\\')\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Pretty print F1\n",
    "print(\"F1 scores, xlm-r span detection:\")\n",
    "print(' & '.join(['tuned lan'] + list(bert_map.keys())) + '\\\\\\\\')\n",
    "for language in bert_map.keys():\n",
    "    print(' & '.join([language] + [f'{metrics[language][language2][\"f1\"]:.2f}' for language2 in bert_map.keys()]) + '\\\\\\\\')\n",
    "print(' & '.join(['Average'] + [f'{sum([metrics[language][language2][\"f1\"] for language2 in bert_map.keys()]) / len(bert_map.keys()):.2f}' for language in bert_map.keys()]) + '\\\\\\\\')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
