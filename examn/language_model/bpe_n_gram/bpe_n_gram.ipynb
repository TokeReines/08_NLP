{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokereines/miniconda3/envs/08_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for arabic using 1-grams in question: 1113.8889298334905\n",
      "Perplexity for arabic using 2-grams in question: 806.2036869725847\n",
      "Perplexity for arabic using 3-grams in question: 2892.8206527586626\n",
      "Perplexity for bengali using 1-grams in question: 1033.544124486604\n",
      "Perplexity for bengali using 2-grams in question: 392.7413803882843\n",
      "Perplexity for bengali using 3-grams in question: 907.161208880907\n",
      "Perplexity for indonesian using 1-grams in question: 1054.0973057955289\n",
      "Perplexity for indonesian using 2-grams in question: 690.5564623641836\n",
      "Perplexity for indonesian using 3-grams in question: 2156.901599065793\n",
      "Perplexity for arabic using 1-grams document: 2361.35061286874\n",
      "Perplexity for arabic using 2-grams document: 1057.5043929730352\n",
      "Perplexity for arabic using 3-grams document: 4318.201644807369\n",
      "Perplexity for bengali using 1-grams document: 2322.6386501576\n",
      "Perplexity for bengali using 2-grams document: 2030.7911510277181\n",
      "Perplexity for bengali using 3-grams document: 5243.607188592675\n",
      "Perplexity for indonesian using 1-grams document: 1979.651081734886\n",
      "Perplexity for indonesian using 2-grams document: 1715.340691101209\n",
      "Perplexity for indonesian using 3-grams document: 5783.023270886824\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams\n",
    "from datasets import load_dataset\n",
    "from bpemb import BPEmb\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "# Define the languages of interest\n",
    "languages = [\"arabic\", \"bengali\", \"indonesian\"]\n",
    "language_to_bpe = {\n",
    "    'bengali': BPEmb(lang=\"bn\", dim=50), \n",
    "    'indonesian': BPEmb(lang=\"id\", dim=50), \n",
    "    'arabic': BPEmb(lang=\"ar\", dim=50)\n",
    "}\n",
    "\n",
    "# Define an order for the n-grams\n",
    "N = 3\n",
    "\n",
    "def tokenize_dataset(dataset, lang, language_to_bpe, question_only=False, document_only=False):\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        if entry['language'] != lang:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if question_only:\n",
    "            text = entry['question_text']\n",
    "        elif document_only:\n",
    "            text = entry['document_plaintext']\n",
    "        else:\n",
    "            text = entry['question_text'] + ' ' + entry['document_plaintext']\n",
    "            \n",
    "        bpe_text = language_to_bpe[lang].encode(text)\n",
    "        tokenized_data.append(bpe_text)\n",
    "    return tokenized_data\n",
    "\n",
    "# Create and evaluate a Laplace model for each language\n",
    "for lang in languages:\n",
    "    for n in range(1, N+1):\n",
    "        tokenized_train = tokenize_dataset(train_set, lang, language_to_bpe, question_only=True)\n",
    "        tokenized_validation = tokenize_dataset(validation_set, lang, language_to_bpe, question_only=True)\n",
    "\n",
    "        train_data, padded_vocab = padded_everygram_pipeline(N, tokenized_train)\n",
    "        validation_data, _ = padded_everygram_pipeline(N, tokenized_validation)\n",
    "\n",
    "        lm = Laplace(n)\n",
    "        lm.fit(train_data, padded_vocab)\n",
    "\n",
    "        # Calculate perplexity on validation data for the current language\n",
    "        validation_ngrams = [ng for sent in tokenized_validation for ng in ngrams(sent, n)]\n",
    "        perplexity = lm.perplexity(validation_ngrams)\n",
    "        print(f\"Perplexity for {lang} using {n}-grams in question: {perplexity}\")\n",
    "        \n",
    "\n",
    "# Create and evaluate a Laplace model for each language\n",
    "for lang in languages:\n",
    "    for n in range(1, N+1):\n",
    "        tokenized_train = tokenize_dataset(train_set, lang, language_to_bpe, document_only=True)\n",
    "        tokenized_validation = tokenize_dataset(validation_set, lang, language_to_bpe, document_only=True)\n",
    "\n",
    "        train_data, padded_vocab = padded_everygram_pipeline(N, tokenized_train)\n",
    "        validation_data, _ = padded_everygram_pipeline(N, tokenized_validation)\n",
    "\n",
    "        lm = Laplace(n)\n",
    "        lm.fit(train_data, padded_vocab)\n",
    "\n",
    "        # Calculate perplexity on validation data for the current language\n",
    "        validation_ngrams = [ng for sent in tokenized_validation for ng in ngrams(sent, n)]\n",
    "        perplexity = lm.perplexity(validation_ngrams)\n",
    "        print(f\"Perplexity for {lang} using {n}-grams document: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
